{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AG-NLP-CH16.ipynb",
      "provenance": [],
      "mount_file_id": "1s8wZDVskpIJID1EgIgeEIHafygn86OAe",
      "authorship_tag": "ABX9TyNLSzNhlCHZk0dI2IBB7Ka3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JpChii/ML-Projects/blob/main/AG_NLP_CH16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvaVJISsqrOJ"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "In this notebook, am going to workout the concepts and techniques discussed in aurelion geron's tensorflow book second edition chapter 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRIgEYHLq_uq"
      },
      "source": [
        "## Creating training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL8b3tVSa5Ng",
        "outputId": "d2a1dfe7-6ae3-4ab3-b2cc-daabf2c564ed"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-a9daf187-9e7c-a9a9-b3b1-0dfa7cc612cc)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW514m7TrLFv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewv4poE2rCfw",
        "outputId": "dc8cd328-f025-4b0d-be73-8ae8882e3a80"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" # shortcut url\n",
        "filepath = keras.utils.get_file(\"input.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFT5KRVzrZs-"
      },
      "source": [
        "Next we must encode every character as an integer, we're going to encoder each character using `Keras Tokenizer` class. This class maps eah character used in the text and maps them to different character id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh1VtR3hrntJ"
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # char level true to use char encoding instead of default word encoding."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3p1zYWsBcE"
      },
      "source": [
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TSxFZHRsRTb"
      },
      "source": [
        "Now the tokenizer can encode a sentence to list of character ID\"s and back and can tell how many distinct characters are there in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxESwYj1sex1",
        "outputId": "489dd3b8-4fc0-4720-d233-8fafb09a2d04"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGduM5EDsi9b",
        "outputId": "ab337307-251e-49da-d2c9-bd4d4ad2544b"
      },
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqMeTlUksrMt"
      },
      "source": [
        "Encode the full text so each character is represented by it's ID( Subract 1 to get IDs from 0 to 38, rather than from 1 to 39, becuase tokenizer starts the encoding from 0 and not 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQUSSKbDs_37"
      },
      "source": [
        "import numpy as np\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFJWwVJJtGNW"
      },
      "source": [
        "Next we need to split the dataset into training, validation and test set. We can't just shuffle since it's sequentail data and we'll lose valuabale information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fNfMdfHtZf1"
      },
      "source": [
        "## How to Split a Sequential Dataset\n",
        "\n",
        "It's important to avoid overlap between the datasets.\n",
        "\n",
        "The splitting of a sequence data is not a trivial task and it soley depends on the problem at hand. Refer page number 528 for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAeYJz82tiCE",
        "outputId": "3291fc1b-0d8b-4ef9-fb51-3cfa50829328"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # macimum number of distinct characters\n",
        "max_id"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97uMc5s9vA7d",
        "outputId": "e3cb0126-3857-479a-c97b-c267f5e2025d"
      },
      "source": [
        "dataset_size = tokenizer.document_count # total number of characters\n",
        "dataset_size"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp1jqFnFvHx3",
        "outputId": "ed1ac2c6-cd72-417e-c2df-f654e35de8d5"
      },
      "source": [
        "train_size = dataset_size * 10 // 100\n",
        "train_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111539"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58--QqTqvN2a"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_9tgvCnvTGN"
      },
      "source": [
        "## Chopping the Sequential dataset into multiple windows\n",
        "\n",
        "The training set now consists of a sequence of over million characters, so we can't just train the neurla network dirctly on it: the RNN would be a equivalent to a deep net with million layers `(**why?** it would be RNN with million layers)` and we would train the net on a single sequence. Instead we'll use `dataset's window()` method to convert this long sequence of text into many smaller window of text and the RNN will be unrolled only over the length of these substrings. This is called `truncated backpropogation through time`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAeDEZjzwNbP"
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weD4WXX33XpS"
      },
      "source": [
        "* `Shift` of `1` creates a dataset like 0 to 101, 1 to 102, 2 to 103 and so on.\n",
        "* `drop_remainder=True` to avoid the last 100 windows which will start decreasing from 100 to 1.\n",
        "\n",
        "The *`window()`* method creates a dataset that contains windows each of which is alsos a datsaset. It's `nested dataset` like list of lists. This is useful when the transformation(batch shuffle) is required for each window. But this can't be passed to the model since expects tensors and not datasets. So we'll use *`flat_map()`* method. It converts a nest dataset into a flat dataset.\n",
        "\n",
        "flat_map() method takes a function as an argument before flattening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj2OAqG55keo"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnnYuNyM5lDE",
        "outputId": "51ce17f7-ac47-4037-8318-47774b349c02"
      },
      "source": [
        "for i in dataset.take(2):\n",
        "  print(i)\n",
        "  print(f\"Shape of flat_map: {i.shape}\")\n",
        "  print(f\"Dimension: {i.ndim}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
            "  0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
            "  4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
            " 17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
            " 10 15  3 13  0], shape=(101,), dtype=int64)\n",
            "Shape of flat_map: (101,)\n",
            "Dimension: 1\n",
            "tf.Tensor(\n",
            "[ 5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1  0\n",
            " 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1  4\n",
            "  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24 17\n",
            "  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10\n",
            " 15  3 13  0  4], shape=(101,), dtype=int64)\n",
            "Shape of flat_map: (101,)\n",
            "Dimension: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF0qKXqE6MsW"
      },
      "source": [
        "So the lambda function has batched windowd_dataset to a `window_length` batch_sized tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeE-NE7z6liq"
      },
      "source": [
        "Since gradient descent wotks best when the instances in the training set are indepedent and identically distributed, we'll shuffle the windows. Then batch the windows and seperate the inputs ( the first 100 characters) from the targer (the last character)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_baFkh1y69sn"
      },
      "source": [
        "batch_size=32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tgV8xIY7WPD",
        "outputId": "166b297c-2298-44b3-ca20-357b21c5f951"
      },
      "source": [
        "for i in dataset.take(1):\n",
        "  print(f\"Length of batch: {len(i)}\")\n",
        "  print(f\"One sample from a batch: {i[0]}\")\n",
        "  print(f\"Shape of one dataset: {i[0].shape}\")\n",
        "  print(f\"One sample from a feature: {i[0][0]}\")\n",
        "  print(f\"Shape of one feature: {i[0][0].shape}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of batch: 2\n",
            "One sample from a batch: [[ 0 16  6 ...  8 23  0]\n",
            " [ 0  6  4 ...  6  4  2]\n",
            " [ 1  0 20 ...  0 22  1]\n",
            " ...\n",
            " [ 6  0  4 ...  6  1  0]\n",
            " [ 8  3 14 ... 15 29 10]\n",
            " [ 2  5  2 ...  2  6  1]]\n",
            "Shape of one dataset: (32, 100)\n",
            "One sample from a feature: [ 0 16  6  5 11  1  0  5  2  0 16  1  8  1 10 16  6  3 11  1  7  3 14  1\n",
            " 17  0 16  1  0 14  5 20  6  2  0 20 13  1  7  7  0  2  6  1 15  0  8  1\n",
            " 11  5  1 25  1 12  0 13  7  0  6 13 14  4  9  1 11 15 28 10 21 13  2  0\n",
            "  2  6  1 15  0  2  6  5  9 24  0 16  1  0  4  8  1  0  2  3  3  0 12  1\n",
            "  4  8 23  0]\n",
            "Shape of one feature: (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-3bvSSp7XqC"
      },
      "source": [
        "Since categorical input features had to be encoded, let's encode them as one-hot vectors. Since there are fairly distince characters( only 39)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGG-AbnQ9L7R"
      },
      "source": [
        "dataset = dataset.map(\n",
        "    lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch)\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn1SjXsy9dXj",
        "outputId": "2c479cc7-8045-4eca-bf79-853943ad1881"
      },
      "source": [
        "for i in dataset.take(1):\n",
        "  print(f\"Length of batch: {len(i)}\")\n",
        "  print(f\"One sample from a batch: {i[0]}\")\n",
        "  print(f\"Shape of one dataset: {i[0].shape}\")\n",
        "  print(f\"One sample from a feature: {i[0][0]}\")\n",
        "  print(f\"Shape of one feature: {i[0][0].shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of batch: 2\n",
            "One sample from a batch: [[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Shape of one dataset: (32, 100, 39)\n",
            "One sample from a feature: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Shape of one feature: (100, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsCWqQwQ9gjo"
      },
      "source": [
        "# Adding prefetching\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPRAdYX99m9G"
      },
      "source": [
        "That's it the dataseet is ready. Let's move on to modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYB6Pobo9sQA"
      },
      "source": [
        "## Building and Training the Char-RNN Model\n",
        "\n",
        "* Two RNN layers with 128 units\n",
        "* 20% dropout on input and hidden states\n",
        "* Time-Distributed dense layer\n",
        "* Softmax activation with 39 units - whihc output 39 probabalities summing up to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA7gFxVC9zJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7c3440-eec7-4862-cce5-0674dbeb2d3e"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "  keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                   dropout=0.2, recurrent_dropout=0.2),\n",
        "  keras.layers.GRU(128, return_sequences=True,\n",
        "                   dropout=0.2, recurrent_dropout=0.2),\n",
        "  keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")),\n",
        "])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "gO_W3Zt0_agM",
        "outputId": "e72fd46e-1a43-4253-fa9d-c898db86e4a4"
      },
      "source": [
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=keras.optimizers.Adam())\n",
        "history = model.fit(dataset, epochs=3,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                                             verbose=1,\n",
        "                                                             restore_best_weights=True)])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-442a3d933db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n\u001b[0m\u001b[1;32m      2\u001b[0m               optimizer=keras.optimizers.Adam())\n\u001b[1;32m      3\u001b[0m history = model.fit(dataset, epochs=3,\n\u001b[1;32m      4\u001b[0m                     callbacks=[keras.callbacks.EarlyStopping(monitor='loss',\n\u001b[1;32m      5\u001b[0m                                                              \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dE1ajBW_qhY"
      },
      "source": [
        "Okay, the training takes 7500secs(120 mins) in an ideal case. I don't have an GPU to perform long computations since this is free tier and get's timed out on idle state. So i'll move along the book without creating this model. Let's skip this and code next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2upOVnTx9gVi"
      },
      "source": [
        "### Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxc2aX_OYbdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a68bc7ad-85bd-43df-de7c-101e591cecb2"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/ML_models/char_rnn.h5\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5a808b0dfcdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ML_models/char_rnn.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FAgoSQyJ5f6"
      },
      "source": [
        "def preprocess(text):\n",
        "  X = np.array(tokenizer.texts_to_sequences(text)) - 1\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kizi23l_YpBa"
      },
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence last char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00JoXYPbnnl9"
      },
      "source": [
        "loaded_mode = keras.models.load_model(\"/content/drive/MyDrive/ML_models/char_rnn.h5\")\n",
        "input = \"How are yo\"\n",
        "print(f\"Input text: {input}\")\n",
        "print(f\"Length of input: {len(input)}\")\n",
        "X_new = preprocess([input])\n",
        "print(f\"Prediction: {model.predict(X_new)}\\n\")\n",
        "print(f\"Prediction shape: {model.predict(X_new).shape}\\n\")\n",
        "Y_pred = np.argmax(loaded_mode.predict(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence last char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-bnl8dKDBL"
      },
      "source": [
        "Generating new text using char-RNN model, we can fieed it some text, make the modek predict the most likely next letter, add it at the end of the text, then guve the extended text to the model to guess the next letter. But in practice this leads to words being repeated.Instead we can pick the next character randomly, with a probablity equal ti the estimated probabality using `tf.random.Categorical()` function. This will generate more diverse and interesting text.\n",
        "\n",
        "The `categorical()` function samples the random class indices, given the log probabalities(logits). To have more control ovver the diversity of the generated text, we can divide the logists by a number called `temperature`, which we can tweak as we wish: a temperature close to 0 will favour the high probabality characters, while avery high temperature will give all characters an equal probabality. The following `next_char()` function uses this approach to pick the next character to add to the input text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq6Yl28ROQYx"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  # print(f\"Prediction: {model.predict(X_new)}\\n\")\n",
        "  # print(f\"Prediction shape: {model.predict(X_new).shape}\\n\")\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples = 1) + 1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0lbhOKLO7qT"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kcue9DjO_4_"
      },
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpD8mxTkn-CA"
      },
      "source": [
        "tf.math.log(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOvq9cz3qW_y"
      },
      "source": [
        "The shakespeare model works best at temperature = 1. To generate more convinving text, we can add more GRU layers hidden units, train for longer some regularization etc. Moreover the model is currently incapabale of learning patterns longer than n_steps, which is just 100 characters. Wich will make training harder and even LSTM and GRU cells cannot handle very long sequences. Alternativley, we can use stateful RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_dfsDTDrbpz"
      },
      "source": [
        "## Stateful RNN\n",
        "\n",
        "Until now, we've used only *stateless RNN* that is the hidden state is resetted to zero after the last time step of every batch like it's not needed. If we told RNN to preserve these hidden states, then the RNN will learn long term depedencies over short sequences. This is called *Stateful RNN*.\n",
        "\n",
        "First thing to note is stateful RNN only makes sense if each input sentence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thning we need to do build a steful RNN is ti use sequential non-overlapping input sequences(rather than shuffled and overlapping sequences used to train stateless RNN's).\n",
        "\n",
        "So we'll use `shift=nsteps`(instead of shift=1) wheen calling the window method and we can't use shuffle since sequences needs to consecutive.\n",
        "\n",
        "And batching using `batch(32)` will be trouble because after batching, the first window of batch1(1) and batch2(33) are not consecutive. The simplest solution to this is to  use `batch(1)` or batches containing a single window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kmJOnLiax-3"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" # shortcut url\n",
        "filepath = keras.utils.get_file(\"input.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ZaBhCKaxxn"
      },
      "source": [
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHD-feJdaxIg"
      },
      "source": [
        "[encoded_full] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeN80FnAbcLG",
        "outputId": "adcf626f-388b-47b0-fb5a-89c0c1ced967"
      },
      "source": [
        "dataset_size_full = tokenizer.document_count\n",
        "dataset_size_full"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2230788"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUX2QIMjbUp3",
        "outputId": "d9dd9e13-2c2d-4b11-d50b-991d3880cd60"
      },
      "source": [
        "train_size_full = dataset_size_full * 90 // 100\n",
        "train_size_full"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2007709"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNwMTdzBFNPx"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded_full[:train_size_full])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JnClix2HyZq"
      },
      "source": [
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        ")\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haLy1t4PIQfy"
      },
      "source": [
        "### Batched dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlpN-i0qIy9x"
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ2tsRTHJR6J",
        "outputId": "c75f75d6-5575-4924-df01-5cda138dbe1f"
      },
      "source": [
        "print(f\"Length of dataset: {len(encoded_full[:train_size_full])}\")\n",
        "encoded_parts = np.array_split(encoded_full[:train_size_full], batch_size)\n",
        "print(f\"Dataset length after split: {len(encoded_parts)}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of dataset: 1115394\n",
            "Dataset length after split: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwEDUyvaJxc9",
        "outputId": "86c5fd75-be61-4371-b54c-fbad7c80fb78"
      },
      "source": [
        "len(encoded_parts[0])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34857"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDo9XB_EKGa6",
        "outputId": "3cfcd74b-83e8-42d8-9fd4-76196c197085"
      },
      "source": [
        "34857 * 32"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115424"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk8AymrBKH6U"
      },
      "source": [
        "So what's been done above is the total lenght of data has been split into 32 equal parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS_PmhUHMv41",
        "outputId": "62fb96a5-1b62-458e-a37c-3b1ec8cc18af"
      },
      "source": [
        "34857 // 100"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "348"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1wRlVulKTEn"
      },
      "source": [
        "batched_ds = []\n",
        "for encoded_part in encoded_parts:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "  # From each encode part 34857(encoded_part length) / 100(window length) - 348 windows will be created\n",
        "  dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "  batched_ds.append(dataset)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5RT84HbK6z3"
      },
      "source": [
        "batched_ds = tf.data.Dataset.zip(tuple(batched_ds)).map(lambda *windows: tf.stack(windows))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXEQ6E1LCJu",
        "outputId": "71cd6dee-ce2f-4beb-9e5d-f621820e6b50"
      },
      "source": [
        "batched_ds"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: (32, None), types: tf.int64>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awqVkyNoXzw7"
      },
      "source": [
        "batched_ds = batched_ds.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "batched_ds = batched_ds.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "batched_ds = batched_ds.prefetch(1)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEDW7lhkX6kW",
        "outputId": "e95e3b81-7540-4ea5-d3c4-86b0013193da"
      },
      "source": [
        "batched_ds"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((32, None, 39), (32, None)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DGKViMpX7uf"
      },
      "source": [
        "Okay the dataset is ready now, let's create a stateful RNN. To make a stateful RNN,\n",
        "\n",
        "* set `stateful= True`\n",
        "* Set `batch_input_shape` in first layer. since the hidden state is preserved for each input sequence in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cp-EVpPYJuk"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "  keras.layers.GRU(128, \n",
        "                   return_sequences=True, \n",
        "                   stateful=True,\n",
        "                   dropout=0.2,\n",
        "                   batch_input_shape=[batch_size, None, max_id]),\n",
        "  keras.layers.GRU(128,\n",
        "                   return_sequences=True,\n",
        "                   stateful=True,\n",
        "                   dropout=0.2),\n",
        "  keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIqpwiarZSGL"
      },
      "source": [
        "At the end of each epoch, we need to reset the states before wego back to beginning of the text, let's write a callback for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5U5In--Zlit"
      },
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZSm-54PZt6o"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI2i0fk1Z4or",
        "outputId": "3969bbd1-d5b5-422e-eb58-fbf687c44387"
      },
      "source": [
        "history = model.fit(batched_ds,\n",
        "                    epochs=50,\n",
        "                    callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "348/348 [==============================] - 7s 14ms/step - loss: 2.5929 - accuracy: 0.2648\n",
            "Epoch 2/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 2.2180 - accuracy: 0.3474\n",
            "Epoch 3/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 2.0904 - accuracy: 0.3805\n",
            "Epoch 4/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 2.0189 - accuracy: 0.4007\n",
            "Epoch 5/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.9711 - accuracy: 0.4134\n",
            "Epoch 6/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.9370 - accuracy: 0.4228\n",
            "Epoch 7/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.9116 - accuracy: 0.4297\n",
            "Epoch 8/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8879 - accuracy: 0.4355\n",
            "Epoch 9/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8707 - accuracy: 0.4399\n",
            "Epoch 10/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8579 - accuracy: 0.4433\n",
            "Epoch 11/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8448 - accuracy: 0.4464\n",
            "Epoch 12/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8335 - accuracy: 0.4493\n",
            "Epoch 13/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8246 - accuracy: 0.4519\n",
            "Epoch 14/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.8154 - accuracy: 0.4539\n",
            "Epoch 15/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.8079 - accuracy: 0.4559\n",
            "Epoch 16/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.8039 - accuracy: 0.4564\n",
            "Epoch 17/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7937 - accuracy: 0.4598\n",
            "Epoch 18/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7901 - accuracy: 0.4602\n",
            "Epoch 19/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7845 - accuracy: 0.4614\n",
            "Epoch 20/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7793 - accuracy: 0.4625\n",
            "Epoch 21/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7752 - accuracy: 0.4637\n",
            "Epoch 22/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7731 - accuracy: 0.4645\n",
            "Epoch 23/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7699 - accuracy: 0.4649\n",
            "Epoch 24/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7638 - accuracy: 0.4670\n",
            "Epoch 25/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7605 - accuracy: 0.4680\n",
            "Epoch 26/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7604 - accuracy: 0.4677\n",
            "Epoch 27/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7568 - accuracy: 0.4684\n",
            "Epoch 28/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7516 - accuracy: 0.4700\n",
            "Epoch 29/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7482 - accuracy: 0.4704\n",
            "Epoch 30/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7442 - accuracy: 0.4720\n",
            "Epoch 31/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7426 - accuracy: 0.4720\n",
            "Epoch 32/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7417 - accuracy: 0.4728\n",
            "Epoch 33/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7405 - accuracy: 0.4724\n",
            "Epoch 34/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7378 - accuracy: 0.4737\n",
            "Epoch 35/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7339 - accuracy: 0.4748\n",
            "Epoch 36/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7329 - accuracy: 0.4748\n",
            "Epoch 37/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7316 - accuracy: 0.4751\n",
            "Epoch 38/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7284 - accuracy: 0.4760\n",
            "Epoch 39/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7280 - accuracy: 0.4761\n",
            "Epoch 40/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7264 - accuracy: 0.4766\n",
            "Epoch 41/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7239 - accuracy: 0.4774\n",
            "Epoch 42/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7244 - accuracy: 0.4770\n",
            "Epoch 43/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7245 - accuracy: 0.4767\n",
            "Epoch 44/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7188 - accuracy: 0.4785\n",
            "Epoch 45/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7187 - accuracy: 0.4785\n",
            "Epoch 46/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7185 - accuracy: 0.4783\n",
            "Epoch 47/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7174 - accuracy: 0.4790\n",
            "Epoch 48/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7154 - accuracy: 0.4796\n",
            "Epoch 49/50\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.7140 - accuracy: 0.4796\n",
            "Epoch 50/50\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 1.7124 - accuracy: 0.4803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRjB9iktaA8A"
      },
      "source": [
        "We can only use this moel to make predictions on batches since it's trained on batches. Let's build a stateless amodel and load the weights in it to predict on single sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrzzBMeDdmNj"
      },
      "source": [
        "stateless_model = keras.Sequential([\n",
        "                                    keras.layers.GRU(128, return_sequences=True),\n",
        "                                    keras.layers.GRU(128, return_sequences=True),\n",
        "                                    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Hfy9Bud-am"
      },
      "source": [
        "# building the model to load weights\n",
        "stateless_model.build(tf.TensorShape([None, None, max_id]))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7lcsHG7eI8_"
      },
      "source": [
        "stateless_model.set_weights(model.get_weights())"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCqiHasweMR9"
      },
      "source": [
        "model = stateless_model"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKfiLBjxeRTn",
        "outputId": "adf015e4-c78e-4706-e261-cb7f2bd14259"
      },
      "source": [
        "print(complete_text(\"t\", temperature=0.3))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that will not so she be seen a business of the hand\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSVM-q7xeVzn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}