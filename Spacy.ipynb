{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBkF88tSGgHNoCTNwORNye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JpChii/ML-Projects/blob/main/Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Finding words, phrases, names, concepts\n",
        "\n",
        "Learn basics of text processing with spacy. Learn about the data structures, how to work with trained pipelines and how to use them to predict linguistic features in your text."
      ],
      "metadata": {
        "id": "VgjgT3RfLOuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spacy\n",
        "import spacy"
      ],
      "metadata": {
        "id": "tLmAkDirPKNN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a blank English nlp object\n",
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "fAwKuH-HPRPk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NLP object contains the processing pipeline\n",
        "* Includes language-spaecific rules for tokenization etc"
      ],
      "metadata": {
        "id": "S-DAoeoRPmnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Doc object\n",
        "doc = nlp(\"Hello world!\")"
      ],
      "metadata": {
        "id": "VFBlqUW5PYiE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over tokens in Doc\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDIn-c1HPlLH",
        "outputId": "0c215b2c-ff2f-44c2-d500-7ebdcc05e878"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "world\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The token object\n",
        "# Index into the Doc to get a single token\n",
        "token = doc[1]"
      ],
      "metadata": {
        "id": "KQpRoUdPPzCf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ_P0vT7P6yO",
        "outputId": "416db0a3-0d2e-4743-a64c-1a81304537e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The span object\n",
        "# A slice from the Doc is a span object\n",
        "span = doc[1:3]\n",
        "print(span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQlxCG8IP75m",
        "outputId": "7ddcf576-09d4-4e31-e055-769afd506bc3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical attributes\n",
        "doc = nlp(\"It costs $5.\")\n",
        "print(f\"Index: {[token.i for token in doc]}\")\n",
        "print(f\"Text: {[token.text for token in doc]}\")\n",
        "\n",
        "print(f\"is_alpha: {[token.is_alpha for token in doc]}\")\n",
        "print(f\"is_punct: {[token.is_punct for token in doc]}\")\n",
        "print(f\"like_num: {[token.like_num for token in doc]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da16541aQRO6",
        "outputId": "483b9a39-1b26-41b0-e597-4bc391aae007"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: [0, 1, 2, 3, 4]\n",
            "Text: ['It', 'costs', '$', '5', '.']\n",
            "is_alpha: [True, True, False, False, False]\n",
            "is_punct: [False, False, False, False, True]\n",
            "like_num: [False, False, False, True, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Documents, spans and tokens"
      ],
      "metadata": {
        "id": "-RhXuUdUQcl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When calling `nlp` on a string, SpaCy first tokenizes the text and creates a document object. In this excercise, we'll lean more about the `Doc`, as well as its views `Token` and `Span`."
      ],
      "metadata": {
        "id": "HpjQUMcNRxSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# preprocess text\n",
        "doc = nlp(\"I like tree kangaroos and narwhals\")\n",
        "\n",
        "# Select the first token\n",
        "first_token = doc[0]\n",
        "\n",
        "# Print the firs token's text\n",
        "print(first_token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvxcXo7xmItI",
        "outputId": "a99104df-44f0-4e7e-e7f9-3e52c2263ed6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Lexical attributes\n",
        "\n",
        "Look for two subsequent tokens: a number and a percent sign"
      ],
      "metadata": {
        "id": "PqH-zwD2mmAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text\n",
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "\n",
        "for token in doc:\n",
        "  if token.like_num:\n",
        "    next_token = doc[token.i + 1]\n",
        "    if next_token.text == \"%\":\n",
        "      print(f\"Percentage found: {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPdOdV7dnlua",
        "outputId": "1c6ce54e-82de-4c69-ba4b-895086bfcc1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage found: 60\n",
            "Percentage found: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Trained pipelines\n",
        "\n",
        "**What are trained pipelines?**\n",
        "\n",
        "* Models that enable spaCy to predit linguistic attributes in context\n",
        "  * Part-of-Speech tags\n",
        "  * Syntactic dependencies\n",
        "  * Named entities\n",
        "\n",
        "* Trained on labeled example tasks\n",
        "* Can be updated with more examples to fine-tune predictions"
      ],
      "metadata": {
        "id": "79Xcr_mtn1Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Loading Pipelines\n",
        "\n",
        "The pipelines used in this course are already pre-installed. For this more details on spaCy's trained pipelines and how to install them on your machine, see the [documentation](https://spacy.io/usage/models)."
      ],
      "metadata": {
        "id": "OkEKhaNtoboD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading small english pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "print(doc.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsJe0c9rqBEp",
        "outputId": "5e54bc6a-87f3-495d-effa-0991b218a778"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Predicting linguistic annotations\n",
        "\n",
        "Now let's try one of spaCy's trained pipeline packages and see its predictions in action."
      ],
      "metadata": {
        "id": "X3QqedguqPcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  token_text = token.text\n",
        "  token_pos = token.pos_\n",
        "  token_dep = token.dep_\n",
        "\n",
        "  print(f\"Token text: {token_text:<12}, POS: {token_pos:<10}, Dependency: {token_dep}\")\n",
        "  print(f\"Dependency expalination: {spacy.explain(token_dep)}\")\n",
        "  print(f\"POS explaination: {spacy.explain(token_pos)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVnOkxg7qiZZ",
        "outputId": "4c4a0f53-9723-4a7a-ff33-23da2195aec3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token text: It          , POS: PRON      , Dependency: nsubj\n",
            "Dependency expalination: nominal subject\n",
            "POS explaination: pronoun\n",
            "\n",
            "Token text: ’s          , POS: VERB      , Dependency: punct\n",
            "Dependency expalination: punctuation\n",
            "POS explaination: verb\n",
            "\n",
            "Token text: official    , POS: NOUN      , Dependency: ccomp\n",
            "Dependency expalination: clausal complement\n",
            "POS explaination: noun\n",
            "\n",
            "Token text: :           , POS: PUNCT     , Dependency: punct\n",
            "Dependency expalination: punctuation\n",
            "POS explaination: punctuation\n",
            "\n",
            "Token text: Apple       , POS: PROPN     , Dependency: nsubj\n",
            "Dependency expalination: nominal subject\n",
            "POS explaination: proper noun\n",
            "\n",
            "Token text: is          , POS: AUX       , Dependency: ROOT\n",
            "Dependency expalination: None\n",
            "POS explaination: auxiliary\n",
            "\n",
            "Token text: the         , POS: DET       , Dependency: det\n",
            "Dependency expalination: determiner\n",
            "POS explaination: determiner\n",
            "\n",
            "Token text: first       , POS: ADJ       , Dependency: amod\n",
            "Dependency expalination: adjectival modifier\n",
            "POS explaination: adjective\n",
            "\n",
            "Token text: U.S.        , POS: PROPN     , Dependency: nmod\n",
            "Dependency expalination: modifier of nominal\n",
            "POS explaination: proper noun\n",
            "\n",
            "Token text: public      , POS: ADJ       , Dependency: amod\n",
            "Dependency expalination: adjectival modifier\n",
            "POS explaination: adjective\n",
            "\n",
            "Token text: company     , POS: NOUN      , Dependency: attr\n",
            "Dependency expalination: attribute\n",
            "POS explaination: noun\n",
            "\n",
            "Token text: to          , POS: PART      , Dependency: aux\n",
            "Dependency expalination: auxiliary\n",
            "POS explaination: particle\n",
            "\n",
            "Token text: reach       , POS: VERB      , Dependency: relcl\n",
            "Dependency expalination: relative clause modifier\n",
            "POS explaination: verb\n",
            "\n",
            "Token text: a           , POS: DET       , Dependency: det\n",
            "Dependency expalination: determiner\n",
            "POS explaination: determiner\n",
            "\n",
            "Token text: $           , POS: SYM       , Dependency: quantmod\n",
            "Dependency expalination: modifier of quantifier\n",
            "POS explaination: symbol\n",
            "\n",
            "Token text: 1           , POS: NUM       , Dependency: compound\n",
            "Dependency expalination: compound\n",
            "POS explaination: numeral\n",
            "\n",
            "Token text: trillion    , POS: NUM       , Dependency: nummod\n",
            "Dependency expalination: numeric modifier\n",
            "POS explaination: numeral\n",
            "\n",
            "Token text: market      , POS: NOUN      , Dependency: compound\n",
            "Dependency expalination: compound\n",
            "POS explaination: noun\n",
            "\n",
            "Token text: value       , POS: NOUN      , Dependency: dobj\n",
            "Dependency expalination: direct object\n",
            "POS explaination: noun\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the docs to print label_ attribute\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5eH0cPFq_be",
        "outputId": "3ddbcb22-afa0-46dd-b0d2-25be2ca0423e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "first ORDINAL\n",
            "U.S. GPE\n",
            "$1 trillion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Predicting named entities in context\n",
        "\n",
        "Models are staistical and not always right. Whether the predictions are correct depends on the training data and the text preprocessed. Let's take a look at an example."
      ],
      "metadata": {
        "id": "qgIOIv8DsL4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the entities\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and label\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "# Get the span for \"iPhone X\"\n",
        "iphone_x = doc[1:3]\n",
        "\n",
        "# Print the span text\n",
        "print(\"Missing entity:\", iphone_x.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhSEMVpZtRL_",
        "outputId": "22954f90-0619-41bf-b73f-2e84d6db3b7a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "Missing entity: iPhone X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Rule-based matching\n",
        "\n",
        "**Why not just regular expressions?**\n",
        "\n",
        "* Match on `Doc` objects, not just strings\n",
        "* Match on tokens and token attributes\n",
        "* Use a model's predictions\n",
        "* Example: \"duck\"(verb) vs \"duck\"(noun)"
      ],
      "metadata": {
        "id": "Gih6YqfptrBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Using the Matcher"
      ],
      "metadata": {
        "id": "4Z5_LKKEt22K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
        "\n",
        "# Intitialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Create a pattern matching two tokens\n",
        "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "\n",
        "# Add pattern to the matcher\n",
        "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
        "\n",
        "# Use the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csGFF12t08Fr",
        "outputId": "d57d9dc1-afb6-4d40-e9af-275e6a8fbff9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: ['iPhone X']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Writing match patterns\n",
        "\n"
      ],
      "metadata": {
        "id": "gkLOdWsJ0-vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"After making the iOS update you won't notice a radical system-wide \"\n",
        "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
        "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
        "    \"some tweaks once you delve a little deeper.\"\n",
        ")\n",
        "\n",
        "# Writing a pattern for full ios versions\n",
        "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
        "\n",
        "# Add the patterns to the matcher\n",
        "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
        "\n",
        "# use the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvVDs6FJ1HZM",
        "outputId": "f16de47b-e6c0-4424-ec30-fbe198d60c11"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total matches found: 3\n",
            "Matches: ['iOS 7', 'iOS 11', 'iOS 10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
        "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
        "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
        "    \"I also need to download Winzip?\"\n",
        ")\n",
        "\n",
        "# Write a pattern that matches a form of \"download\" plus proper noun\n",
        "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24oE8JKb3URm",
        "outputId": "e1242c62-1c7b-44a0-c6ee-f82b57b89e71"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total matches found: 3\n",
            "Match found: downloaded Fortnite\n",
            "Match found: downloading Minecraft\n",
            "Match found: download Winzip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
        "    \"labels and optional voice responses.\"\n",
        ")\n",
        "\n",
        "# Patter for adjective plus one or two noun\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
        "\n",
        "# Add the pattern to the macher\n",
        "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches and print span text\n",
        "for match_id, start, end in matches:\n",
        "  print(f\"Match found: {doc[start:end]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s86t3w-g5MHI",
        "outputId": "3a23eff7-18a6-4dbc-e6b4-1d9752c16a80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match found: beautiful design\n",
            "Match found: smart search\n",
            "Match found: automatic labels\n",
            "Match found: optional voice\n",
            "Match found: optional voice responses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large-Scale data analysis with spaCy"
      ],
      "metadata": {
        "id": "IIsUd6K358jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data structures(1)\n",
        "\n",
        "### 1.1 Strings to hashes\n",
        "\n",
        "To be memory optimized spaCy stores strings as hashes in stringStore."
      ],
      "metadata": {
        "id": "xjS9lKF-044l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"I have a cat\")\n",
        "\n",
        "# Look up the hash for the word \"cat\"\n",
        "cat_hash = nlp.vocab.strings[\"cat\"]\n",
        "print(cat_hash)\n",
        "\n",
        "# And hash to string\n",
        "cat_string = nlp.vocab.strings[cat_hash]\n",
        "print(cat_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpKgy1Kz3PiD",
        "outputId": "8b68737f-b9f3-4670-fb1e-4c80fcd99b0a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5439657043933447811\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the same for another\n",
        "doc = nlp(\"David Bowie is a PERSON\")\n",
        "\n",
        "person_hash = doc.vocab.strings[\"PERSON\"]\n",
        "print(person_hash)\n",
        "\n",
        "person_string = doc.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jInsSG23sc5",
        "outputId": "597e469a-062b-41fd-e9f9-3d3a5ff8e82a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "380\n",
            "PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "person_hash = doc.vocab.strings[\"CHECK\"]\n",
        "print(person_hash)\n",
        "\n",
        "person_string = doc.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "0y2a3sdq41PQ",
        "outputId": "fe219538-8738-419f-ab40-7aac9a377b4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4398250953217862582\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b9d59ed6cfb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mperson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '4398250953217862582'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hash will return an error for a word it hasn't seen or hashed. So add the word to the vocab or use the same vocab to resolve the hash to back to a string."
      ],
      "metadata": {
        "id": "ERqAvjVQ5Ei6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc.vocab.strings.add(\"CHECK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kopzGfK42vO",
        "outputId": "59f819a8-9eb3-415a-fb7e-be29490dbb17"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4398250953217862582"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "person_hash = doc.vocab.strings[\"CHECK\"]\n",
        "print(person_hash)\n",
        "\n",
        "person_string = doc.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u7v7t_N47q4",
        "outputId": "eb2489d1-3ff4-40c8-8722-8b81f5ac69a3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4398250953217862582\n",
            "CHECK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Structures(2): Doc, Span and Token\n",
        "\n",
        "The Doc Object - Create document from tokens\n",
        "\n",
        "Span Object - Create span document from Doc object\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "* `Doc` and `Span` are very powerful and hold references and relationships of words and sentences\n",
        "  * Convert result to strings as late as possible\n",
        "  * Use token attributes if available - for example, `token.i` for the token index\n",
        "\n",
        "* Don't forget to pass in the shared `vocab`"
      ],
      "metadata": {
        "id": "LibPcYff5DWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a Doc"
      ],
      "metadata": {
        "id": "Atc7MyI97tID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Desied text: \"Spacy is interesting!\"\n",
        "\n",
        "words = [\"spaCy\", \"is\", \"interesting\", \"!\"]\n",
        "spaces = [True, True, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQLAiO6C72UD",
        "outputId": "3438b0b1-458c-4468-8bf9-b9e7aa5b0b46"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy is interesting!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
        "spaces = [False, True, True, False, False]\n",
        "\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5bIi3cg8WT6",
        "outputId": "f21c1675-0612-484b-8acb-95be50a3d2f3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go, get started!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
        "spaces = [False, True, False, False, False]\n",
        "\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHxDAhX89Koa",
        "outputId": "40867268-a323-405b-9fd9-e1367a4d6d4d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, really?!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Docs, spans and entities from scratch\n",
        "\n",
        "Let's createa the `Doc` and `Span` objects manually and update the named entities just like spacy does behind the scenes."
      ],
      "metadata": {
        "id": "G5OkBoz5JJZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Import Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create doc from words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span_with_label = Span(doc, 2, 4, label=\"PERSON\")\n",
        "print(span_with_label.text, span_with_label.label_)\n",
        "doc.ents = [span_with_label]\n",
        "\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vzVxajG9q4b",
        "outputId": "f4f02800-9308-4409-9405-a007517357ad"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like David Bowie\n",
            "David Bowie PERSON\n",
            "[('David Bowie', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hash, word in doc.ents:\n",
        "  print(match_id, start, end)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf3ZCdL9KczS",
        "outputId": "2f899866-3395-4217-b7b4-4aec1b420894"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5488211386492616699 David Bowie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[2:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmSQUFPwK6Pt",
        "outputId": "0850f4e5-f08a-4e2e-91a9-b4a5655f9a7b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "David Bowie"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Data structures and best practices"
      ],
      "metadata": {
        "id": "sO0wmCCKLC6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* token.i gives the index\n",
        "* token.pos_ gives pos\n",
        "* token.dep_ gives dependency of the word"
      ],
      "metadata": {
        "id": "DRVJrReWOIwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Berlin looks like a nice city\")\n",
        "\n",
        "for token in doc:\n",
        "\n",
        "  if token.pos_ == \"PROPN\":\n",
        "\n",
        "    if doc[token.i + 1].pos_ == \"VERB\":\n",
        "      result = doc[token.i + 1]\n",
        "      print(f\"Found proper noun before a verb: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY5M5-SSNJQW",
        "outputId": "432071fc-f537-4fa7-bab3-904029e259ae"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found proper noun before a verb: looks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHzrhvMeNR_i",
        "outputId": "516745b1-759a-436e-d75f-2cc4d90c1285"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nsubj\n",
            "ROOT\n",
            "prep\n",
            "det\n",
            "amod\n",
            "pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Word vectors and semantic similarity\n",
        "\n",
        "* spaCy can compare two objects and predict similarity\n",
        "* `Doc.similarity()` `Span.similarity()` and `Token.similarity()`\n",
        "* Take another object and return a similarity score (`0` to `1`)\n",
        "* Needs a pipeline that has word vectors included\n",
        "\n",
        "### 3.1 Inspecting word vectors\n",
        "\n",
        "We'll use a larger English pipeline, which revolves around 20,000 word vectors."
      ],
      "metadata": {
        "id": "kJ9NT99vNSVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSriVCx5SoDt",
        "outputId": "20168f3a-ceea-4aab-80df-d2aa1871c5c3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051301 sha256=3c756321c345f2597ffb9e911504fdf678a8fe2fc1a84f18785af2fec7bec5a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dsqw1c39/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.cli.download(\"en_core_web_md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-i0e77UTGOP",
        "outputId": "7643dbe4-6095-4d44-81ac-230be2c3d842"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(\"Two bananas in pyjamas\")\n",
        "bananas_vector = doc[1].vector\n",
        "print(bananas_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_MIypiDQjBj",
        "outputId": "9451e972-24ee-466c-98b3-838ac0ac82f3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
            " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
            "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
            "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
            "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
            "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
            "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
            " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
            "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
            "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
            " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
            "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
            "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
            "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
            " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
            "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
            " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
            " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
            "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
            " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
            " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
            "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
            " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
            " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
            " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
            "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
            "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
            "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
            "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
            " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
            "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
            " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
            "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
            " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
            "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
            "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
            " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
            "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
            " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
            " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
            " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
            "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
            " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
            "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
            " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
            " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
            "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
            "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
            "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
            " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Comparing similarities"
      ],
      "metadata": {
        "id": "8gsNHJh2Th9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc similarities\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc1 = nlp(\"It's a warm summer day\")\n",
        "doc2 = nlp(\"It's sunny outside\")\n",
        "\n",
        "# Get similarity \n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsqS0-TZT1A-",
        "outputId": "4aa84d3c-8d64-4f9c-c3bb-280026ea2a6c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8789265574516525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token similarity\n",
        "doc = nlp(\"TV and books\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMKAhazPUQQf",
        "outputId": "fcc1e3cc-dc9e-45dc-fc52-d26bd6729684"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22325331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Span similarity\n",
        "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "span1 = doc[3:5]\n",
        "span2 = doc[12:15]\n",
        "\n",
        "similarity = span1.similarity(span2)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxT4zKOwUfe-",
        "outputId": "0c202095-2dc9-40f3-8711-6b52c384917b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75173926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For custom nlp problems, we might have to train our own vectors."
      ],
      "metadata": {
        "id": "WNpTA4GLVVt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Combining predictions and rules\n",
        "\n",
        "| |Statistical models|Rule-based systems|\n",
        "|-------|------------------|----------|\n",
        "|Use cases|\tapplication needs to generalize based on examples|dictionary with finite number of examples|\n",
        "|Real-world examples|\tproduct names, person names, subject/object |relationships\tcountries of the world, cities, drug names, dog breeds|\n",
        "|spaCy features|\tentity recognizer, dependency parser, part-of-speech tagger|\ttokenizer, Matcher, PhraseMatcher|"
      ],
      "metadata": {
        "id": "addQSGuSU3Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\n",
        "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
        "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
        "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
        "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
        "    \"Prime for new members, beginning on September 14. However, members with \"\n",
        "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
        "    \"viewing until their subscription comes up for renewal. Those with \"\n",
        "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
        ")\n",
        "\n",
        "# Create the match patterns\n",
        "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
        "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "# Initialize the Matcher and add the patterns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PATTERN1\", [pattern1])\n",
        "matcher.add(\"PATTERN2\", [pattern2])\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print pattern string name and text of matched span\n",
        "    print(doc.vocab.strings[match_id], doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flZZj1e6VEWr",
        "outputId": "37c43a9b-8265-456b-8ef6-d15f3cbe777e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PATTERN1 Amazon Prime\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN1 Amazon Prime\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN2 ad-free viewing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bWgLWsaOYRRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}