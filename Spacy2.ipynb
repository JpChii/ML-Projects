{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd56cf70",
   "metadata": {},
   "source": [
    "This is continution of [part1 notebook](https://github.com/JpChii/ML-Projects/blob/main/Spacy.ipynb)\n",
    "\n",
    "# Processing Pipelines\n",
    "\n",
    "Learn everything needed about spaCy's processing pipeline. Learn what goes uner the hood when processing a text and to write custom components and add them to pipeline, custom attributes to add own metadata to the documents, spans and tokens.\n",
    "\n",
    "![alt text](https://course.spacy.io/pipeline.png \"What happens when nlp is called?\")\n",
    "\n",
    "**Built-in pipeline components**\n",
    "\n",
    "|Name|Description|Creates|\n",
    "|----|-----------|-------|\n",
    "| tagger | Part-of-speech tagger | Token.tag, Token.pos |\n",
    "| parser | Dependency parser | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| ner | Named entity recogonizer | Doc.ents, Token.ent_iob, Token.ent_type |\n",
    "| textcat | Text classifier | Doc.cats |\n",
    "\n",
    "**Under the hood**\n",
    "![alt text](https://course.spacy.io/package_meta.png \"What happens when nlp is called?\")\n",
    "\n",
    "* Pipeline defined in model's `config.cfg` in order\n",
    "* Built-inn components need binary data to make predictions\n",
    "\n",
    "### 1.1 Inspecting the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e36a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in ./env/lib/python3.7/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.0)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./env/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./env/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in ./env/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./env/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in ./env/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23937921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f78b8376750>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f78b8376980>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f78bc6593d0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f78bc6fea50>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f78bc6ffd70>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f78bc6594d0>)]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipeline)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ace460",
   "metadata": {},
   "source": [
    "## 2. Custom Pipeline components\n",
    "\n",
    "**why custom components?**\n",
    "\n",
    "* Make a function execute automatically when calling nlp\n",
    "* Add own metadata to documents and tokens\n",
    "* Updating built-in attributes like `doc.ents`\n",
    "\n",
    "### 2.1 Simple component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc3cd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 2 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Tst in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ccabbb",
   "metadata": {},
   "source": [
    "### 2.2 Complex components\n",
    "\n",
    "Write a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8becc4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "\n",
    "print(\"animal_patterns\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d024cc",
   "metadata": {},
   "source": [
    "Yay!! Written a custom pipeline component for rule-based entity matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee544b57",
   "metadata": {},
   "source": [
    "## 3. Extension attributes\n",
    "\n",
    "* Add custom metadata to documents, tokens and spans\n",
    "* Accessible via the ._ property\n",
    "\n",
    "**Extension attributes types**\n",
    "\n",
    "1. Attribute extensions\n",
    "2. Property extensions\n",
    "3. Method extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fc432",
   "metadata": {},
   "source": [
    "### 3.1 Setting extension attributes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19c2f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Attribute extension\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False, force=True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cd42625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Getter function extension\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Defining the getter function\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(f\"reversed: {token._.reversed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2159a",
   "metadata": {},
   "source": [
    "### 3.2 Setting extension attributes (2)\n",
    "\n",
    "Set an extension function for the `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9b5ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: functools.partial(<function get_has_number at 0x7f78bb1585f0>, The museum closed for five years in 2012.)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "Doc.set_extension(\"has_number\", method=get_has_number, force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(f\"has_number: {doc._.has_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "986afd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html, force=True)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8e469",
   "metadata": {},
   "source": [
    "### 3.3 Entities and extensions\n",
    "\n",
    "Getting a wikipedia url if the span's label is in the list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "528ce20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David-Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "Span.set_extension(\"wiki_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.wiki_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1c264",
   "metadata": {},
   "source": [
    "Yay!! Created a pipeline component that uses named entities predicted by the model to generate wikipedia URLs and adds them as a custom attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ef33069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.blank(\"en\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59eaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"captial\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96896202",
   "metadata": {},
   "source": [
    "This is an example of how to add structured data to a spaCy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb6d97",
   "metadata": {},
   "source": [
    "## 4. Scaling and Performance\n",
    "\n",
    "**Processing large volumes of text**\n",
    "\n",
    "* Use `nlp.pipe` method\n",
    "* Preprocesses texts as a stream, yields `Doc` objects\n",
    "* Much faster than calling `nlp` on each text\n",
    "\n",
    "**Passing in context(1)**\n",
    "\n",
    "* Setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
    "* Yields `(doc, context)` tuples\n",
    "* Useful for associating metadata with `doc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affe14f",
   "metadata": {},
   "source": [
    "### 4.1 Processing streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "504c26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"data/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d2f5f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all the tweet contents alone\n",
    "tweets = [tweet[\"content\"] for tweet in TEXTS]\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8e8e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(tweets[:20]):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77321d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "# docs = [nlp(text) for text in TEXTS] # Bad performance\n",
    "docs = list(nlp.pipe(docs)) # Good perforance\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649467c",
   "metadata": {},
   "source": [
    "### 4.3 Preprocessing with context\n",
    "\n",
    "In this exercise, youâ€™ll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of [text, context] examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys \"author\" and \"book\".\n",
    "\n",
    "Use the set_extension method to register the custom attributes \"author\" and \"book\" on the Doc, which default to None.\n",
    "Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "Overwrite the doc._.book and doc._.author with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"exercises/en/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n â€” '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e6066",
   "metadata": {},
   "source": [
    "### 4.4 Selecective processing\n",
    "\n",
    "In this excercise, you'll use the `nlp.make_doc` and `nlp.select_pipes` method to run only selected components when running a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e3176c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Performing only toenization\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dba72369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55143f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# Running only the ner disabling tagger and lemmetizer\n",
    "with nlp.select_pipes(enable=\"ner\"):\n",
    "    doc = nlp(doc)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c84cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
