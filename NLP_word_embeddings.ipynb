{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-word-embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCwR4mU8/HrFHk0ic+GWhl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JpChii/ML-Projects/blob/main/NLP_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAojoYkYzUWE"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "Machine Learning models like numbers and they hate anythinh other than numbers they really do. To get useful information via machine learning, first step is to convert text data into numbers. This post discusses about few methods,\n",
        "\n",
        "## One-Hot Encoding\n",
        "\n",
        "Encoding text to numbers. Create a zero vector to the length of the vocabulary(number of unique words on the data) and assign `1` at the index o the word. By doing this what we achieve is called a sparse vector, meaning most indices of the vector are zeros. To form a sentence, concatenate one-hot encoding of the words.\n",
        "\n",
        "Let's consider a vocabulary with 15,000 words and we encode all of them, what we get is 99.99% of zeros in our data which is really inefficient for training.\n",
        "\n",
        "## Integer Encoding with unique numbers\n",
        "\n",
        "Let's switch to use an unique number for each words in the vocabulary. This is efficient thean the above because we'll get a dense vector instead of sparse vector.\n",
        "\n",
        "But in this method, we lose valuable information to amke something out of the data. Realationship between words is lost. Integer encoding can be challenging for models to interpret, because there is no relationship between similar words and the encodings are alos differnet. This leads to feature-weight combination which is not meaningful.\n",
        "\n",
        "This where **embedding** comes in\n",
        "\n",
        "## Word Embeddings\n",
        "\n",
        "Word embedding is an efficient dense vector way where similar words have similar encodings. Embeddings are floating point vectors, whose values doesn't need to be setup manually. The advantage is the embedding values are learned during training similar to weights of a dense layer. The length of the vector is a parameter to be specified.\n",
        "\n",
        "The embedding length ranges from 8-dimensional for small datasets to 1024-dimensions for large datasets but requires more data to learn.\n",
        "\n",
        "In this notebook, we'll use [IMDB review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) to practice word embedding and then perform sentiment analysis on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygjTr0mCz4DT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}