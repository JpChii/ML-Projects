{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e4aee3",
   "metadata": {},
   "source": [
    "This is continution of [part1 notebook](https://github.com/JpChii/ML-Projects/blob/main/Spacy.ipynb)\n",
    "\n",
    "# Processing Pipelines\n",
    "\n",
    "Learn everything needed about spaCy's processing pipeline. Learn what goes uner the hood when processing a text and to write custom components and add them to pipeline, custom attributes to add own metadata to the documents, spans and tokens.\n",
    "\n",
    "![alt text](https://course.spacy.io/pipeline.png \"What happens when nlp is called?\")\n",
    "\n",
    "**Built-in pipeline components**\n",
    "\n",
    "|Name|Description|Creates|\n",
    "|----|-----------|-------|\n",
    "| tagger | Part-of-speech tagger | Token.tag, Token.pos |\n",
    "| parser | Dependency parser | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| ner | Named entity recogonizer | Doc.ents, Token.ent_iob, Token.ent_type |\n",
    "| textcat | Text classifier | Doc.cats |\n",
    "\n",
    "**Under the hood**\n",
    "![alt text](https://course.spacy.io/package_meta.png \"What happens when nlp is called?\")\n",
    "\n",
    "* Pipeline defined in model's `config.cfg` in order\n",
    "* Built-inn components need binary data to make predictions\n",
    "\n",
    "### 1.1 Inspecting the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in ./env/lib/python3.7/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./env/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: zipp>=0.5 in ./env/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./env/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in ./env/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./env/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in ./env/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9f6356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7faa5e7bf6e0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7faa5e7bf600>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7faa5eb2a450>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7faa5ebfc960>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7faa5ebf8780>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7faa5eadfd50>)]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipeline)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7081c",
   "metadata": {},
   "source": [
    "## 2. Custom Pipeline components\n",
    "\n",
    "**why custom components?**\n",
    "\n",
    "* Make a function execute automatically when calling nlp\n",
    "* Add own metadata to documents and tokens\n",
    "* Updating built-in attributes like `doc.ents`\n",
    "\n",
    "### 2.1 Simple component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d81135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 2 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Tst in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78130cb7",
   "metadata": {},
   "source": [
    "### 2.2 Complex components\n",
    "\n",
    "Write a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3488bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "\n",
    "print(\"animal_patterns\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d7bf4",
   "metadata": {},
   "source": [
    "Yay!! Written a custom pipeline component for rule-based entity matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4daa8",
   "metadata": {},
   "source": [
    "## 3. Extension attributes\n",
    "\n",
    "* Add custom metadata to documents, tokens and spans\n",
    "* Accessible via the ._ property\n",
    "\n",
    "**Extension attributes types**\n",
    "\n",
    "1. Attribute extensions\n",
    "2. Property extensions\n",
    "3. Method extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aac5b1",
   "metadata": {},
   "source": [
    "### 3.1 Setting extension attributes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f48630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Attribute extension\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False, force=True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31217192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Getter function extension\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Defining the getter function\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(f\"reversed: {token._.reversed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19dc41",
   "metadata": {},
   "source": [
    "### 3.2 Setting extension attributes (2)\n",
    "\n",
    "Set an extension function for the `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76eeb552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: functools.partial(<function get_has_number at 0x7faa6279bef0>, The museum closed for five years in 2012.)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "Doc.set_extension(\"has_number\", method=get_has_number, force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(f\"has_number: {doc._.has_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0d4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html, force=True)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52877018",
   "metadata": {},
   "source": [
    "### 3.3 Entities and extensions\n",
    "\n",
    "Getting a wikipedia url if the span's label is in the list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892ce26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "Span.set_extension(\"wiki_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.wiki_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c3e00",
   "metadata": {},
   "source": [
    "Yay!! Created a pipeline component that uses named entities predicted by the model to generate wikipedia URLs and adds them as a custom attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e94c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.blank(\"en\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "841b2f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exercises/en/countries.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s2/zc28s499001f26bz7nbfmfhr0000gn/T/ipykernel_920/1846845597.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exercises/en/countries.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mCOUNTRIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercises/en/countries.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"captial\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d9234",
   "metadata": {},
   "source": [
    "This is an example of how to add structured data to a spaCy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9418cf",
   "metadata": {},
   "source": [
    "## 4. Scaling and Performance\n",
    "\n",
    "**Processing large volumes of text**\n",
    "\n",
    "* Use `nlp.pipe` method\n",
    "* Preprocesses texts as a stream, yields `Doc` objects\n",
    "* Much faster than calling `nlp` on each text\n",
    "\n",
    "**Passing in context(1)**\n",
    "\n",
    "* Setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
    "* Yields `(doc, context)` tuples\n",
    "* Useful for associating metadata with `doc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c70b7",
   "metadata": {},
   "source": [
    "### 4.1 Processing streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"data/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551aa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the tweet contents alone\n",
    "tweets = [tweet[\"content\"] for tweet in TEXTS]\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b35d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(tweets[:20]):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc74f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "# docs = [nlp(text) for text in TEXTS] # Bad performance\n",
    "docs = list(nlp.pipe(docs)) # Good perforance\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f21435",
   "metadata": {},
   "source": [
    "### 4.3 Preprocessing with context\n",
    "\n",
    "In this exercise, youâ€™ll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of [text, context] examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys \"author\" and \"book\".\n",
    "\n",
    "Use the set_extension method to register the custom attributes \"author\" and \"book\" on the Doc, which default to None.\n",
    "Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "Overwrite the doc._.book and doc._.author with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"exercises/en/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n â€” '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80043f80",
   "metadata": {},
   "source": [
    "### 4.4 Selecective processing\n",
    "\n",
    "In this excercise, you'll use the `nlp.make_doc` and `nlp.select_pipes` method to run only selected components when running a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Performing only toenization\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running only the ner disabling tagger and lemmetizer\n",
    "with nlp.select_pipes(enable=\"ner\"):\n",
    "    doc = nlp(doc)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fa75e",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "1. What goes behing an nlp pipeline call\n",
    "2. Inspection the pipeline components using pipe_name and pipe\n",
    "3. Custom PhraseMatcher to include structured data for natches and using the matches and assigning the spans to entities. Adding custom components to existing pipeline\n",
    "4. Extension attributes, methods(getter)\n",
    "5. Components with extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5259d0",
   "metadata": {},
   "source": [
    "# Training a neural network model\n",
    "\n",
    "Learn how to update spaCy's statistical models to customize thme for use case. For example to predict a new entity type in online comments. Train our own model from scratch and understand the basic of how training works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1316a",
   "metadata": {},
   "source": [
    "## Training and updating models\n",
    "\n",
    "**Why update the model?**\n",
    "\n",
    "* Better results on specific domain\n",
    "* Lean classification schemes specifically for a problem\n",
    "* Essential for text classification\n",
    "* Very useful for named entity recogonition\n",
    "* Less critical for part-of-speech tagging and dependency parsing\n",
    "\n",
    "**How training works (1)**\n",
    "\n",
    "* Initialize the model weights randomly\n",
    "* Predict a few examples with the current weights\n",
    "* Compare predictions with true labels\n",
    "* Calculate how to change weights to improve predictions\n",
    "* Update weights slightly\n",
    "* go back to step 2\n",
    "\n",
    "**How training works (2)**\n",
    "![alt text](https://course.spacy.io/training.png)\n",
    "\n",
    "* Training data: Examples and their annotations\n",
    "* Text: The inputtext the model should predict a label for\n",
    "* Label: The label the model should predict\n",
    "* Gradient: How to change the weights\n",
    "\n",
    "**Example: Training the entity recogonizer**\n",
    "\n",
    "* The entity recogonizer tags words and phrases in context\n",
    "* Each token can only be part of one entity\n",
    "* Examples needs to come with context\n",
    "* Text with no entitites are also important\n",
    "* Goal: Teach the model to generalize\n",
    "\n",
    "**The training data**\n",
    "\n",
    "* Examples of what we want the model to predict in context\n",
    "* Update an *existing model*: a few hundred toa few thousand examples\n",
    "* Train a *new category*: a few thousand to million examples\n",
    "    * spaCy's English models: 2 million words\n",
    "* Usually create manually by human annotators\n",
    "* Can be semi-automates - for example, using spaCy's `Matcher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7373b0cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  \"How to preorder the iPhone X\",\r\n",
      "  \"iPhone X is coming\",\r\n",
      "  \"Should I pay $1,000 for the iPhone X?\",\r\n",
      "  \"The iPhone 8 reviews are here\",\r\n",
      "  \"iPhone 11 vs iPhone 8: What's the difference?\",\r\n",
      "  \"I need a new phone! Any tips?\"\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/iphone.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5f761",
   "metadata": {},
   "source": [
    "## 1. Training and evaluation data\n",
    "\n",
    "### 1.1 Creating Training data (1)\n",
    "\n",
    "spaCy's rule-based `Matcher` is a great way to quicky create training data for named entity models. A list of sentences is available as the variable TEXTS. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recogonize them as \"GADGET\".\n",
    "\n",
    "* Write a pattern for two tokens whose lowercase forms match `iphone` and `x`\n",
    "* Write a pattern for two tokens: one token whose lowercase matches `iphone` and a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b957d94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "with open(\"data/iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "    \n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "docs = []\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c770af",
   "metadata": {},
   "source": [
    "Nice! Let's use those patterns to quickly bootstap some training data for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6c3de",
   "metadata": {},
   "source": [
    "### 1.2 Create training data (2)\n",
    "\n",
    "After creating the data for our corpus, we need to save it out to a `.spacy`file. The code from the previous example is already available\n",
    "\n",
    "* Instantiate the `DocBin` with the list of `docs`.\n",
    "* Save the `DocBin` to a file called `train.spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82dc618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[How to preorder the iPhone X,\n",
       " iPhone X is coming,\n",
       " Should I pay $1,000 for the iPhone X?,\n",
       " The iPhone 8 reviews are here,\n",
       " iPhone 11 vs iPhone 8: What's the difference?,\n",
       " I need a new phone! Any tips?]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "768a26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"data/train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916c483",
   "metadata": {},
   "source": [
    "Well well well! The pipeline is now ready, so let's start writing the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba715bee",
   "metadata": {},
   "source": [
    "## 2. Configuring and running the training\n",
    "\n",
    "**The training config (1)**\n",
    "\n",
    "* *single source of truth* for all settings\n",
    "* typically called `config.cfg`\n",
    "* defines how to initialize the `nlp` object\n",
    "* includes all settings abot the pipeline components and their model implementations\n",
    "* Configures the training process and hyperparameters\n",
    "* makes training more reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be49f16",
   "metadata": {},
   "source": [
    "### 2.1 Generating a config file\n",
    "\n",
    "The `init config` command auto-generates a config file for training with the default settings. We want to train a named entity recogonizer, so let's generate a config file for one pipeline component, `ner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6abba26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3mâš  To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2mâœ” Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2mâœ” Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04118028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\r\n",
      "train = null\r\n",
      "dev = null\r\n",
      "vectors = null\r\n",
      "init_tok2vec = null\r\n",
      "\r\n",
      "[system]\r\n",
      "gpu_allocator = null\r\n",
      "seed = 0\r\n",
      "\r\n",
      "[nlp]\r\n",
      "lang = \"en\"\r\n",
      "pipeline = [\"tok2vec\",\"ner\"]\r\n",
      "batch_size = 1000\r\n",
      "disabled = []\r\n",
      "before_creation = null\r\n",
      "after_creation = null\r\n",
      "after_pipeline_creation = null\r\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\r\n",
      "\r\n",
      "[components]\r\n",
      "\r\n",
      "[components.ner]\r\n",
      "factory = \"ner\"\r\n",
      "incorrect_spans_key = null\r\n",
      "moves = null\r\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\r\n",
      "update_with_oracle_cut_size = 100\r\n",
      "\r\n",
      "[components.ner.model]\r\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\r\n",
      "state_type = \"ner\"\r\n",
      "extra_state_tokens = false\r\n",
      "hidden_width = 64\r\n",
      "maxout_pieces = 2\r\n",
      "use_upper = true\r\n",
      "nO = null\r\n",
      "\r\n",
      "[components.ner.model.tok2vec]\r\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\r\n",
      "width = ${components.tok2vec.model.encode.width}\r\n",
      "upstream = \"*\"\r\n",
      "\r\n",
      "[components.tok2vec]\r\n",
      "factory = \"tok2vec\"\r\n",
      "\r\n",
      "[components.tok2vec.model]\r\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\r\n",
      "\r\n",
      "[components.tok2vec.model.embed]\r\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\r\n",
      "width = ${components.tok2vec.model.encode.width}\r\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\r\n",
      "rows = [5000,2500,2500,2500]\r\n",
      "include_static_vectors = false\r\n",
      "\r\n",
      "[components.tok2vec.model.encode]\r\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\r\n",
      "width = 96\r\n",
      "depth = 4\r\n",
      "window_size = 1\r\n",
      "maxout_pieces = 3\r\n",
      "\r\n",
      "[corpora]\r\n",
      "\r\n",
      "[corpora.dev]\r\n",
      "@readers = \"spacy.Corpus.v1\"\r\n",
      "path = ${paths.dev}\r\n",
      "max_length = 0\r\n",
      "gold_preproc = false\r\n",
      "limit = 0\r\n",
      "augmenter = null\r\n",
      "\r\n",
      "[corpora.train]\r\n",
      "@readers = \"spacy.Corpus.v1\"\r\n",
      "path = ${paths.train}\r\n",
      "max_length = 0\r\n",
      "gold_preproc = false\r\n",
      "limit = 0\r\n",
      "augmenter = null\r\n",
      "\r\n",
      "[training]\r\n",
      "dev_corpus = \"corpora.dev\"\r\n",
      "train_corpus = \"corpora.train\"\r\n",
      "seed = ${system.seed}\r\n",
      "gpu_allocator = ${system.gpu_allocator}\r\n",
      "dropout = 0.1\r\n",
      "accumulate_gradient = 1\r\n",
      "patience = 1600\r\n",
      "max_epochs = 0\r\n",
      "max_steps = 20000\r\n",
      "eval_frequency = 200\r\n",
      "frozen_components = []\r\n",
      "annotating_components = []\r\n",
      "before_to_disk = null\r\n",
      "\r\n",
      "[training.batcher]\r\n",
      "@batchers = \"spacy.batch_by_words.v1\"\r\n",
      "discard_oversize = false\r\n",
      "tolerance = 0.2\r\n",
      "get_length = null\r\n",
      "\r\n",
      "[training.batcher.size]\r\n",
      "@schedules = \"compounding.v1\"\r\n",
      "start = 100\r\n",
      "stop = 1000\r\n",
      "compound = 1.001\r\n",
      "t = 0.0\r\n",
      "\r\n",
      "[training.logger]\r\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\r\n",
      "progress_bar = false\r\n",
      "\r\n",
      "[training.optimizer]\r\n",
      "@optimizers = \"Adam.v1\"\r\n",
      "beta1 = 0.9\r\n",
      "beta2 = 0.999\r\n",
      "L2_is_weight_decay = true\r\n",
      "L2 = 0.01\r\n",
      "grad_clip = 1.0\r\n",
      "use_averages = false\r\n",
      "eps = 0.00000001\r\n",
      "learn_rate = 0.001\r\n",
      "\r\n",
      "[training.score_weights]\r\n",
      "ents_f = 1.0\r\n",
      "ents_p = 0.0\r\n",
      "ents_r = 0.0\r\n",
      "ents_per_type = null\r\n",
      "\r\n",
      "[pretraining]\r\n",
      "\r\n",
      "[initialize]\r\n",
      "vectors = ${paths.vectors}\r\n",
      "init_tok2vec = ${paths.init_tok2vec}\r\n",
      "vocab_data = null\r\n",
      "lookups = null\r\n",
      "before_init = null\r\n",
      "after_init = null\r\n",
      "\r\n",
      "[initialize.components]\r\n",
      "\r\n",
      "[initialize.tokenizer]"
     ]
    }
   ],
   "source": [
    "!cat ./config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa9481",
   "metadata": {},
   "source": [
    "### 2.2 Using the training CLI\n",
    "\n",
    "Let' use the config gile generated in the previous excercise and the training corpus we've created to train the named entity recognizer!\n",
    "\n",
    "The `train` command lets us to train a model from a training config file. A file `config_gadget.cfg` contains training config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9159ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2730  100  2730    0     0   5645      0 --:--:-- --:--:-- --:--:--  5783\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/explosion/spacy-course/master/exercises/en/config_gadget.cfg --output config_gadget.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd96870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Created output directory: output\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-04-16 13:06:25,759] [INFO] Set up nlp object from config\n",
      "[2022-04-16 13:06:25,776] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-04-16 13:06:25,789] [INFO] Created vocabulary\n",
      "[2022-04-16 13:06:25,793] [INFO] Finished initializing nlp object\n",
      "[2022-04-16 13:06:27,069] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2mâœ” Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     20.33    1.69    1.04    4.44    0.02\n",
      "/Users/jayaprakashsivagami/Documents/Tech/ML/Projects/env/lib/python3.7/site-packages/thinc/layers/layernorm.py:32: RuntimeWarning: invalid value encountered in reciprocal\n",
      "  d_xhat = N * dY - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n",
      "  1     200         29.20    986.03   76.92   76.09   77.78    0.77\n",
      "  2     400         74.01    246.38   81.28   78.35   84.44    0.81\n",
      "  4     600         56.61    115.50   78.72   75.51   82.22    0.79\n",
      "  6     800         66.39     74.29   87.29   86.81   87.78    0.87\n",
      "  9    1000         84.28     55.62   84.95   82.29   87.78    0.85\n",
      " 12    1200         41.63     30.49   83.33   83.33   83.33    0.83\n",
      " 16    1400         61.41     30.56   83.33   83.33   83.33    0.83\n",
      " 22    1600        121.04     23.78   85.39   86.36   84.44    0.85\n",
      " 28    1800         80.12     21.55   82.61   80.85   84.44    0.83\n",
      " 36    2000         47.95      9.97   81.52   79.79   83.33    0.82\n",
      " 46    2200        110.66     23.58   84.15   82.80   85.56    0.84\n",
      " 58    2400         47.54      8.16   84.32   82.11   86.67    0.84\n",
      "\u001b[38;5;2mâœ” Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config_gadget.cfg --output ./output --paths.train data/train_gadget.spacy --paths.dev data/dev_gadget.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db0c7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mmodel-best\u001b[m\u001b[m \u001b[34mmodel-last\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9446a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
